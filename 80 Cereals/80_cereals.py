# -*- coding: utf-8 -*-
"""80_Cereals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QG1Itu-y93dpWFhd122YcTHM-LT2B2sG
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

file_path = "/content/drive/MyDrive/Colab Notebooks/80_Cereals/cereal.csv"
cereal = pd.read_csv(file_path)
cereal.head()

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
axes[0].scatter(cereal['sugars'], cereal['calories'])
axes[0].set_xlabel('Sugar')
axes[0].set_ylabel('Calories')
axes[1].scatter(cereal['fiber'], cereal['calories'])
axes[1].set_xlabel('Fiber')
axes[1].set_ylabel('Calories')
axes[2].scatter(cereal['protein'], cereal['calories'])
axes[2].set_xlabel('Protein')
axes[2].set_ylabel('Calories')

input_data = cereal[['sugars','fiber','protein','calories']]
input_data

input_data = torch.tensor(np.array(input_data), dtype=torch.float)
input_data[:, 0:3]

class LinearModel(nn.Module):
  def __init__(self):
    super(LinearModel, self).__init__()
    self.linear = nn.Linear(3, 1)  # Define a linear layer with an input size of 3 and an output size of 1

  def forward(self, x):
    return self.linear(x)

model = LinearModel()

output = model(input_data[:, 0:3])
output.shape

# Define the loss function
loss_function = nn.MSELoss()

# Define the optimizer as SGD
optimizer = optim.SGD(model.parameters(), lr=0.001)

X = input_data[:, 0:3]
y = input_data[:, 3]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

epochs = 100

for epoch in range(epochs):
  # Forward Pass
  output = model(X_train)

  # Calculate loss
  loss = loss_function(output, y_train)

  # Backward Pass
  optimizer.zero_grad()
  loss.backward()

  # Update the hyperparameters
  optimizer.step()

  # Print the steps
  if (epoch+1)%10 == 0:
    print(f"On epoch {epoch+1}/{epochs}, Loss: {loss.item()}")

# Evaluate the model created
model.eval()

with torch.no_grad():
  output_test = model(X_test)
  mse_torch = loss_function(output_test, y_test)
  mse_sklearn = mean_squared_error(output_test, y_test)

print(f"Mean Squared Error (MSE) from torch: {mse_torch}")
print(f"Mean Squared Error (MSE) from sklearn: {mse_sklearn}")

# Keep training the model

extra_epochs = 200
model.train()

for epoch in range(101, extra_epochs+1, 1):

  # Calculate outputs and loss
  output = model(X_train)
  loss = loss_function(output, y_train)

  # Backward pass
  optimizer.zero_grad()
  loss.backward()

  # Update the hyperparameters
  optimizer.step()

  if (epoch % 10) == 0:
    print(f"On epoch {epoch} / {extra_epochs}, the loss is {loss.item()}")

# Evaluate the model created again
model.eval()

with torch.no_grad():
  output_test = model(X_test)
  mse_torch = loss_function(output_test, y_test)
  mse_sklearn = mean_squared_error(output_test, y_test)

print(f"Mean Squared Error (MSE) from torch: {mse_torch}")
print(f"Mean Squared Error (MSE) from sklearn: {mse_sklearn}")

